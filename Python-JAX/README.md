Welcome to python's JAX.

JAX, or Just in time Autograd and XLA, is a numpy-like module that allows us to create and implement mathematical operations to perform on arrays and objects for data manipulation and ML patterns learning. This module is great when working with GPUs and TPUs, since the content given is compiled into bytecode instead of just scripted or interpreted. The following roadmap will be worked upon: 

    A[ðŸ§  Core Python & Math Refresher] --> B[ðŸ”¢ Phase 1: NumPy (Local Learning)]
    B --> C[ðŸ“ Phase 2: Linear Algebra with NumPy]
    C --> D[ðŸ“Š Phase 3: Build ML from Scratch w/ NumPy]
    D --> E[âš™ï¸ Phase 4: Intro to JAX in Google Colab]
    E --> F[âš¡ Phase 5: JAX Core â€“ grad, jit, vmap]
    F --> G[ðŸ§  Phase 6: Rebuild ML Projects in JAX]
    G --> H[ðŸŒ± Phase 7: Learn Flax or Haiku for NN Architectures]
    H --> I[ðŸš€ Phase 8: Advanced JAX: Custom Training Loops & Accelerators]

    style B fill:#E3F2FD
    style E fill:#FFF3E0